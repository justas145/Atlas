{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "# mixtral:8x22b-instruct\n",
    "# llama3:70b-instruct-q8_0\n",
    "# dolphin-llama3:70b-v2.9-q8_0\n",
    "local_llm = 'dolphin-llama3:70b-v2.9-q8_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Respond to the human as helpfully and accurately as possible. You have access to the following tools:\\n\\n{tools}\\n\\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\n\\nValid \"action\" values: \"Final Answer\" or {tool_names}\\n\\nProvide only ONE action per $JSON_BLOB, as shown:\\n\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\nFollow this format:\\n\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\n```\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_model_names(base_url):\n",
    "    api_endpoint = f\"{base_url}/api/tags\"\n",
    "    try:\n",
    "        response = requests.get(api_endpoint)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "        data = response.json()\n",
    "        model_names = [model['name'] for model in data.get('models', [])]\n",
    "        return model_names\n",
    "    except requests.RequestException as e:\n",
    "        return str(e)\n",
    "\n",
    "# Example usage\n",
    "base_url = \"https://ollama.junzis.com\"\n",
    "model_names = fetch_model_names(base_url)\n",
    "print(model_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixtral:8x22b-instruct\n",
    "# llama3:70b-instruct-q8_0\n",
    "# dolphin-llama3:70b-v2.9-q8_0\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, tool, create_tool_calling_agent, create_openai_tools_agent\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "local_llm = 'llama3:70b-instruct-q8_0'\n",
    "ollama_llm = OllamaFunctions(base_url='https://ollama.junzis.com',\n",
    "                             model=local_llm, temperature=0.0)\n",
    "\n",
    "\n",
    "ollama_llm.invoke('Hello, how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# take environment variables from .env so we can load OPENAI_API_KEY\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "json_schema = {\n",
    "    \"title\": \"equation\",\n",
    "    \"description\": \"information about an equation.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"first_number\": {\"title\": \"First Number\", \"description\": \"first number in the equation\", \"type\": \"string\"},\n",
    "        \"second_number\": {\"title\": \"Second Number\", \"description\": \"Second number in the equation\", \"type\": \"string\"},\n",
    "        \"operation\": {\"title\": \"Operation\", \"description\": \"operation to perform. Options: +, -, *, /\", \"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"first_number\", \"second_number\", \"operation\"],\n",
    "}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are extracting information in structured formats.\"),\n",
    "        (\"human\",\n",
    "         \"Use the given format to extract information from the following input: {input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "#model = OllamaFunctions(model='phi3:latest', format=\"json\")\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = create_structured_output_chain(\n",
    "    json_schema, model, prompt, verbose=True)\n",
    "\n",
    "chain.run('Here I need to check how much is 3243 divided by 3. (i should use a calculator for this)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "@tool\n",
    "def calculator(equation: str):\n",
    "    \"\"\"Simple 2 number calculator\"\"\"\n",
    "    parsed_equation = chain.run(equation)\n",
    "    print(parsed_equation)\n",
    "    num1 = float(parsed_equation['first_number'])\n",
    "    num2 = float(parsed_equation['second_number'])\n",
    "    operation = str(parsed_equation['operation'])\n",
    "    if operation == '+':\n",
    "        return str(num1 + num2)\n",
    "    elif operation == '-':\n",
    "        return str(num1 - num2)\n",
    "    elif operation == '*':\n",
    "        return str(num1 * num2)\n",
    "    elif operation == '/':\n",
    "        return str(num1 / num2)\n",
    "    else:\n",
    "        return \"Invalid operation\"\n",
    "\n",
    "\n",
    "\n",
    "tools = [calculator, TavilySearchResults()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_llm = ChatOllama(base_url='https://ollama.junzis.com',\n",
    "                        model='dolphin-llama3:70b-v2.9-q8_0', temperature=0.0)\n",
    "ollama_llm.invoke('Hello, how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent, create_json_chat_agent\n",
    "\n",
    "agent_prompt = hub.pull(\"hwchase17/react-chat-json\")\n",
    "\n",
    "\n",
    "# ollama_llm = OllamaFunctions(base_url='https://ollama.junzis.com',\n",
    "#                         model='dolphin-llama3:70b-v2.9-q8_0', temperature=0.0, format='json')\n",
    "# ollama_llm = ChatOpenAI(base_url='https://ollama.junzis.com/v1',\n",
    "#                         model='dolphin-llama3:70b-v2.9-q8_0', temperature=0.0, api_key='ollama')\n",
    "ollama_llm = ChatOllama(base_url='http://ollama.junzis.com',\n",
    "                        model='dolphin-llama3:70b-v2.9-q8_0', temperature=0.0)\n",
    "agent = create_json_chat_agent(ollama_llm, tools, agent_prompt)\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "agent_executor.invoke({\"input\": \"search how much an elephant weighs in pounds and then multiply it by 146376 to get the answer1, then search how much does a an A320 airplane weight and divide it by 20 to get answer2. Finally sum up answer1 and answer2 to get the answer.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "\n",
    "def call_tools(msg: AIMessage) -> Runnable:\n",
    "    \"\"\"Simple sequential tool calling helper.\"\"\"\n",
    "    tool_map = {tool.name: tool for tool in tools}\n",
    "    tool_calls = msg.tool_calls.copy()\n",
    "    for tool_call in tool_calls:\n",
    "        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(\n",
    "            tool_call[\"args\"])\n",
    "    return tool_calls\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"Add two integers.\"\n",
    "    return first_int + second_int\n",
    "\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"Exponentiate the base to the exponent power.\"\n",
    "    return base**exponent\n",
    "\n",
    "\n",
    "tools = [multiply, exponentiate, add]\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "ollama_with_tools = ollama_llm.bind_tools(\n",
    "    tools=functions, function_call=functions)\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "\n",
    "chain2 = ollama_with_tools | call_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ollama_llm.bind_tools(\n",
    "    tools=functions,\n",
    "    \n",
    ")\n",
    "model.invoke(\"what is 2 + 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = model | call_tools\n",
    "chain2.invoke(\"what is 2 + 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_with_tools.invoke(\"What is 2 to the power of 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_call = {t['name']: t for t in functions}\n",
    "function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2.invoke(\"What's 23 times 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"jlvdoorn/whisper-large-v3-atco2-asr\"\n",
    "# model_id = \"scy0208/whisper-aviation-base\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "\n",
    "def split_mp3(path):\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_mp3(path)\n",
    "\n",
    "    # Calculate the duration of the audio in seconds\n",
    "    duration = len(audio) / 1000  # pydub uses milliseconds\n",
    "\n",
    "    # Create a directory for the split files\n",
    "    base_path, filename = os.path.split(path)\n",
    "    file_stem, _ = os.path.splitext(filename)\n",
    "    output_dir = os.path.join(base_path, file_stem)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Split the audio file into 10-second chunks\n",
    "    for i in range(0, int(duration), 10):\n",
    "        # Calculate end time for each chunk\n",
    "        end_time = min((i + 10) * 1000, len(audio))\n",
    "        chunk = audio[i * 1000:end_time]\n",
    "\n",
    "        # Define the filename for each chunk\n",
    "        chunk_filename = f\"{file_stem}_part_{i//10 + 1}.mp3\"\n",
    "        chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "\n",
    "        # Export the chunk to a new file\n",
    "        chunk.export(chunk_path, format=\"mp3\")\n",
    "\n",
    "    print(f\"Files are saved in: {output_dir}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "split_mp3(\"C:\\\\Users\\\\justa\\\\Downloads\\\\crashKSEE-Twr-Apr-29-2024-0100Z.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the directory\n",
    "directory_path = \"C:\\\\Users\\\\justa\\\\Downloads\\\\crashKSEE-Twr-Apr-29-2024-0100Z\"\n",
    "files = [f for f in os.listdir(directory_path) if f.endswith('.mp3')]\n",
    "\n",
    "# Sort files to ensure correct order\n",
    "files.sort()\n",
    "\n",
    "# Loop over sorted files\n",
    "for file in files:\n",
    "    # Construct full file path\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "\n",
    "    # Process the file through the pipeline\n",
    "\n",
    "    result = pipe(file_path)\n",
    "\n",
    "    # Print the transcribed text\n",
    "    print(result['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
