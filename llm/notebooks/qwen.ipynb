{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import requests\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_model_names(base_url):\n",
    "    api_endpoint = f\"{base_url}/api/tags\"\n",
    "    try:\n",
    "        response = requests.get(api_endpoint)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "        data = response.json()\n",
    "        model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n",
    "        return model_names\n",
    "    except requests.RequestException as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "base_url = \"https://ollama.junzis.com\"\n",
    "model_names = fetch_model_names(base_url)\n",
    "model = model_names[0]\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../bluesky\")\n",
    "from bluesky.network.client import Client\n",
    "\n",
    "client = Client()\n",
    "client.connect(\"127.0.0.1\", 11000, 11001)\n",
    "client.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import urllib.parse\n",
    "import json5\n",
    "from qwen_agent.agents import Assistant\n",
    "from qwen_agent.tools.base import BaseTool, register_tool\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "from langchain_core.tools import tool as langchain_tool\n",
    "\n",
    "\n",
    "# Step 1 (Optional): Add a custom tool named `my_image_gen`.\n",
    "@register_tool(\"my_image_gen\")\n",
    "class MyImageGen(BaseTool):\n",
    "    # The `description` tells the agent the functionality of this tool.\n",
    "    description = \"AI painting (image generation) service, input text description, and return the image URL drawn based on text information.\"\n",
    "    # The `parameters` tell the agent what input parameters the tool has.\n",
    "    parameters = [\n",
    "        {\n",
    "            \"name\": \"prompt\",\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Detailed description of the desired image content, in English\",\n",
    "            \"required\": True,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        # `params` are the arguments generated by the LLM agent.\n",
    "        prompt = json5.loads(params)[\"prompt\"]\n",
    "        prompt = urllib.parse.quote(prompt)\n",
    "        return json5.dumps(\n",
    "            {\"image_url\": f\"https://image.pollinations.ai/prompt/{prompt}\"},\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def capture_stdout():\n",
    "    new_stdout = StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = new_stdout\n",
    "    try:\n",
    "        yield new_stdout\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "def receive_bluesky_output():\n",
    "    complete_output = \"\"\n",
    "    empty_output_count = 0  # Track consecutive empty outputs\n",
    "\n",
    "    while True:\n",
    "        with capture_stdout() as captured:\n",
    "            client.update()\n",
    "        new_output = captured.getvalue()\n",
    "\n",
    "        # Check if the current output is empty\n",
    "        if not new_output.strip():\n",
    "            empty_output_count += 1  # Increment counter for empty outputs\n",
    "        else:\n",
    "            empty_output_count = 0  # Reset counter if output is not empty\n",
    "            complete_output += new_output  # Add non-empty output to complete output\n",
    "\n",
    "        # If there are two consecutive empty outputs, break the loop\n",
    "        if empty_output_count >= 5:\n",
    "            break\n",
    "\n",
    "    # It's assumed you want to keep the last update outside the loop\n",
    "    client.update()\n",
    "\n",
    "    return complete_output\n",
    "\n",
    "\n",
    "@register_tool(\"get_conflict_info\")\n",
    "class Conflict(BaseTool):\n",
    "    # The `description` tells the agent the functionality of this tool.\n",
    "    description = \"Get conflict information between aircraft pairs. It gives you Time to Closest Point of Approach (TCPA), Quadrantal Direction (QDR), separation distance, Closest Point of Approach distance (DCPA), and Time of Loss of Separation (tLOS).\"\n",
    "    # The `parameters` tell the agent what input parameters the tool has.\n",
    "    parameters = [\n",
    "        {\n",
    "            \"name\": \"command\",\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Command to get conflict information between aircraft pairs. Default is 'SHOWTCPA'\",\n",
    "            \"required\": True,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        # `params` are the arguments generated by the LLM agent.\n",
    "        command = json5.loads(params)[\"command\"]\n",
    "        command = urllib.parse.quote(command)\n",
    "        client.send_event(b\"STACK\", \"SHOWTCPA\")\n",
    "        time.sleep(0.8)\n",
    "        sim_output = receive_bluesky_output()\n",
    "        return json5.dumps(\n",
    "            {\"conflict_info\": sim_output},\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_tool(\"get_aircraft_info\")\n",
    "class Conflict(BaseTool):\n",
    "    # The `description` tells the agent the functionality of this tool.\n",
    "    description = \"Get all aircraft information in the simulation. It gives you aircraft ID, altitude, heading, speed, and position.\"\n",
    "    # The `parameters` tell the agent what input parameters the tool has.\n",
    "    parameters = [\n",
    "        {\n",
    "            \"name\": \"command\",\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Command to get aircraft information. Default is 'GETACIDS'\",\n",
    "            \"required\": True,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        # `params` are the arguments generated by the LLM agent.\n",
    "        command = json5.loads(params)[\"command\"]\n",
    "        command = urllib.parse.quote(command)\n",
    "        client.send_event(b\"STACK\", \"GETACIDS\")\n",
    "        time.sleep(0.8)\n",
    "        sim_output = receive_bluesky_output()\n",
    "        return json5.dumps(\n",
    "            {\"aircraft_info\": sim_output},\n",
    "            ensure_ascii=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Configure the LLM you are using.\n",
    "llm_cfg = {\n",
    "    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:\n",
    "    \"model\": \"qwen2:72b\",\n",
    "    \"model_server\": \"https://ollama.junzis.com/v1\",  # base_url, also known as api_base\n",
    "    \"api_key\": \"EMPTY\",\n",
    "    # (Optional) LLM hyperparameters for generation:\n",
    "    # \"generate_cfg\": {\"top_p\": 0.8},\n",
    "}\n",
    "\n",
    "# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.\n",
    "system_instruction = \"\"\"You are a helpful assistant.\"\"\"\n",
    "tools = [\n",
    "    \"get_conflict_info\",\n",
    "    \"get_aircraft_info\",\n",
    "]  # `code_interpreter` is a built-in tool for executing code.\n",
    "bot = Assistant(\n",
    "    llm=llm_cfg, system_message=system_instruction, function_list=tools\n",
    ")\n",
    "# Step 4: Run the agent as a chatbot.\n",
    "# messages = []  # This stores the chat history.\n",
    "# while True:\n",
    "#     # For example, enter the query \"draw a dog and rotate it 90 degrees\".\n",
    "#     query = input(\"user query: \")\n",
    "#     # Append the user query to the chat history.\n",
    "#     messages.append({\"role\": \"user\", \"content\": query})\n",
    "#     response = []\n",
    "#     for response in bot.run(messages=messages):\n",
    "#         # Streaming output.\n",
    "#         print(\"bot response:\")\n",
    "#         pprint.pprint(response, indent=2)\n",
    "#     # Append the bot responses to the chat history.\n",
    "#     messages.extend(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bot_and_collect_messages(bot, messages):\n",
    "    output = bot.run(messages=messages)\n",
    "    final_responses = []\n",
    "    try:\n",
    "        while True:\n",
    "            response = next(output)\n",
    "            final_responses.extend(response)\n",
    "    except StopIteration:\n",
    "        pass  # The generator has no more output\n",
    "    return final_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = run_bot_and_collect_messages(bot, [{\"role\": \"user\", \"content\": \"please check if there are any conflicts between aircrafts\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"please check if there are any conflicts between aircrafts and if so give me a conflict resolution plan according to ICAO standards. ICAO standards: minimum 5 nm lateral separation or 1000 ft vertical separation\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"please check if there are any conflicts between aircrafts and if so give me a conflict resolution plan according to ICAO standards. ICAO standards: minimum 5 nm lateral separation or 1000 ft vertical separation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "while True:\n",
    "    # For example, enter the query \"draw a dog and rotate it 90 degrees\".\n",
    "    # query = input(\"user query: \")\n",
    "    query = \"please check if there are any conflicts between aircrafts and if so give me a conflict resolution plan according to ICAO standards. ICAO standards: minimum 5 nm lateral separation or 1000 ft vertical separation\"\n",
    "    # Append the user query to the chat history.\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "    response = []\n",
    "    for response in bot.run(messages=messages):\n",
    "        # Streaming output.\n",
    "        print(\"bot response:\")\n",
    "        pprint.pprint(response, indent=2)\n",
    "    # Append the bot responses to the chat history.\n",
    "    messages.extend(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*_, last = bot.run(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"tell me a joke\",\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in bot.run(messages=messages):\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
