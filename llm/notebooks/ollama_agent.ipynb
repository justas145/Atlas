{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import requests\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_model_names(base_url):\n",
    "    api_endpoint = f\"{base_url}/api/tags\"\n",
    "    try:\n",
    "        response = requests.get(api_endpoint)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "        data = response.json()\n",
    "        model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n",
    "        return model_names\n",
    "    except requests.RequestException as e:\n",
    "        return str(e)\n",
    "\n",
    "base_url = \"https://ollama.junzis.com\"\n",
    "model_names = fetch_model_names(base_url)\n",
    "model = model_names[0]\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(base_url=\"https://ollama.junzis.com\", model=model)\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    # Open the image file\n",
    "    with Image.open(image_path) as image:\n",
    "        # Convert the image to RGB format (to ensure compatibility)\n",
    "        rgb_image = image.convert(\"RGB\")\n",
    "        # Create a BytesIO object to hold the image data\n",
    "        buffered = io.BytesIO()\n",
    "        # Save the image to the BytesIO object as JPEG (or PNG if you prefer)\n",
    "        rgb_image.save(buffered, format=\"JPEG\")\n",
    "        # Retrieve the byte data from the buffer\n",
    "        image_byte_data = buffered.getvalue()\n",
    "        # Encode the byte data to Base64\n",
    "        base64_encoded_image = base64.b64encode(image_byte_data)\n",
    "        # Convert bytes to string\n",
    "        base64_string = base64_encoded_image.decode(\"utf-8\")\n",
    "    return base64_string\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_path = \"C:\\\\Users\\\\justa\\\\Downloads\\\\4-ac-conflict.jpg\"  # Replace this with your image file path\n",
    "base64_code = encode_image_to_base64(image_path)\n",
    "print(base64_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL images to Base64 encoded strings\n",
    "\n",
    "    :param pil_image: PIL image\n",
    "    :return: Base64 string\n",
    "    \"\"\"\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"JPEG\")  # You can change the format if needed\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Function to create the prompt with text and image\n",
    "def prompt_func(data):\n",
    "    text = data[\"text\"]\n",
    "    image = data[\"image\"]\n",
    "\n",
    "    image_part = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image}\",\n",
    "    }\n",
    "\n",
    "    content_parts = []\n",
    "\n",
    "    text_part = {\"type\": \"text\", \"text\": text}\n",
    "\n",
    "    content_parts.append(image_part)\n",
    "    content_parts.append(text_part)\n",
    "\n",
    "    return [HumanMessage(content=content_parts)]\n",
    "\n",
    "# Create the chain with the prompt function, model, and output parser\n",
    "chain = prompt_func | llm | StrOutputParser()\n",
    "\n",
    "# Load and convert the image to base64\n",
    "file_path = \"C:\\\\Users\\\\justa\\\\Downloads\\\\4-ac-conflict-larger.jpg\"\n",
    "pil_image = Image.open(file_path)\n",
    "image_b64 = convert_to_base64(pil_image)\n",
    "\n",
    "# Invoke the chain with the text and image data\n",
    "query_chain = chain.invoke(\n",
    "    {\"text\": \"You are seeing the image of aircrafts in conflict. Orange means aircraft in conflict and green means it is not in conflict. Each arrow has an aircraft call sign, flight level FL and speed. Based on the image decide if there is an aircraft conflict and if so make a conflict resolution plan according to ICAO standards: at least 1000ft vertical seperation or 5 NM horizontal seperation\", \"image\": image_b64}\n",
    ")\n",
    "\n",
    "print(query_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(list1, list2):\n",
    "    # Convert lists to vectors\n",
    "    vector1 = np.array(list1)\n",
    "    vector2 = np.array(list2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = dot(vector1, vector2) / (norm(vector1) * norm(vector2))\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "similarity_score = cosine_similarity(ac4, ac2_headon)\n",
    "print(f\"Cosine Similarity: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = cosine_similarity(ac2_headon_2, ac2_headon)\n",
    "print(f\"Cosine Similarity: {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaFunctions(model=\"phi3:latest\", format=\"json\", keep_alive=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke('hi there!, tell why 2+2 is 5?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent, create_openai_tools_agent\n",
    "import random\n",
    "from langchain_core.tools import tool\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "ollama_model = ChatOpenAI(model=\"llama3.1:latest\", base_url=\"http://localhost:11434/v1\")\n",
    "# ollama_model.invoke(\"hi\")\n",
    "gpt_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "@tool\n",
    "def fake_weather_api(city: str) -> str:\n",
    "    \"\"\"Check the weather in a specified city. The API is available randomly, approximately every third call.\"\"\"\n",
    "\n",
    "    if random.randint(1, 3) == 1:  # Approximately one-third chance\n",
    "        return \"Sunny, 22°C\"\n",
    "    else:\n",
    "        return \"Service temporarily unavailable\"\n",
    "\n",
    "\n",
    "tools = [fake_weather_api]\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "agent = create_openai_tools_agent(gpt_model, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)\n",
    "out = agent_executor.invoke({\"input\": \"What is the weather in New York? and what is the weather in London?\"})\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent, create_openai_tools_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import random\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tools import tool\n",
    "import random\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "\n",
    "@tool\n",
    "def fake_weather_api(city: str) -> str:\n",
    "    \"\"\"Check the weather in a specified city. The API is available randomly, approximately every third call.\"\"\"\n",
    "\n",
    "    if random.randint(1, 3) == 1:  # Approximately one-third chance\n",
    "        return \"Sunny, 22°C\"\n",
    "    else:\n",
    "        return \"Service temporarily unavailable\"\n",
    "\n",
    "\n",
    "tools = [fake_weather_api]\n",
    "functions = [convert_to_openai_function(tool) for tool in tools]\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Construct the Tools agent\n",
    "agent = create_tool_calling_agent(model, functions, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"what is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the weather for a location\n",
    "\n",
    "    \"\"\"\n",
    "    return \"weather in \" + location + \" is sunny with a high of 75 degrees\"\n",
    "\n",
    "@tool\n",
    "def get_population(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the population of a location\n",
    "\n",
    "    \"\"\"\n",
    "    return \"the population of \" + location + \" is 100,000\"\n",
    "\n",
    "\n",
    "tools = [get_weather, get_population]\n",
    "functions = [convert_to_openai_function(t) for t in tools]\n",
    "tool_mapping = {\"get_weather\": get_weather, \"get_population\": get_population}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind_tools(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"what is the weather in Boston and its population?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.additional_kwargs[\"function_call\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.additional_kwargs | itemgetter(\"arguments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
