{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import tool, initialize_agent, AgentType, Tool\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append('../../bluesky')  # Adjust the path as necessary\n",
    "\n",
    "\n",
    "# Now you can import bluesky modules\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "import bluesky\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "\n",
    "    format_to_openai_tool_messages,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from bluesky.network.client import Client\n",
    "\n",
    "from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "\n",
    "from prompts.prompts import conflict_prompt\n",
    "\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from langchain.agents import Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-3-large\"\n",
    ")\n",
    "vectordb_path = 'C:/Users/justa/OneDrive/Desktop/Developer/LLM-Enhanced-ATM/llm/skills-library/vectordb'\n",
    "chroma_client = chromadb.PersistentClient(path=vectordb_path)\n",
    "# Get a collection object from an existing collection, by name. If it doesn't exist, create it.\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"test2\", embedding_function=openai_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture output information from the bluesky client and return it as a string\n",
    "@contextmanager\n",
    "def capture_stdout():\n",
    "    new_stdout = StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = new_stdout\n",
    "    try:\n",
    "        yield new_stdout\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "def update_until_complete(client):\n",
    "    complete_output = \"\"\n",
    "    empty_output_count = 0  # Track consecutive empty outputs\n",
    "\n",
    "    while True:\n",
    "        with capture_stdout() as captured:\n",
    "            client.update()\n",
    "        new_output = captured.getvalue()\n",
    "\n",
    "        # Check if the current output is empty\n",
    "        if not new_output.strip():\n",
    "            empty_output_count += 1  # Increment counter for empty outputs\n",
    "        else:\n",
    "            empty_output_count = 0  # Reset counter if output is not empty\n",
    "            complete_output += new_output  # Add non-empty output to complete output\n",
    "\n",
    "        # If there are two consecutive empty outputs, break the loop\n",
    "        if empty_output_count >= 2:\n",
    "            break\n",
    "\n",
    "    # It's assumed you want to keep the last update outside the loop\n",
    "    client.update()\n",
    "\n",
    "    return complete_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to client\n",
    "client = Client()\n",
    "client.connect(\"127.0.0.1\", 11000, 11001)\n",
    "client.update()\n",
    "client.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../prompts/basecmds.txt', 'r') as file:\n",
    "    base_cmds = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tools for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get all aircraft info\n",
    "# get conflict information\n",
    "\n",
    "@tool\n",
    "def GetAllAircraftInfo(command: str = 'GETACIDS'):\n",
    "    \"\"\"Get each aircraft information at current time: position, heading (deg), track (deg), altitude, V/S (vertical speed), calibrated, true and ground speed and mach number. Input is 'GETACIDS'.\n",
    "    \n",
    "    Parameters:\n",
    "    - command: str (default 'GETACIDS')\n",
    "    \n",
    "    Example usage:\n",
    "    - GetAllAircraftInfo('GETACIDS')\n",
    "    \n",
    "    Returns:\n",
    "    - str: all aircraft information\n",
    "    \"\"\"\n",
    "    command = command.replace('\"', '').replace(\"'\", \"\")\n",
    "    command = command.split('\\n')[0]\n",
    "    print(f'LLM input:{command}')\n",
    "    \n",
    "    client.send_event(b'STACK', command)\n",
    "    time.sleep(1)\n",
    "    sim_output = update_until_complete(client)\n",
    "    return sim_output\n",
    "\n",
    "@tool\n",
    "def GetConflictInfo(commad: str = 'SHOWTCPA'):\n",
    "    \"\"\"Use this tool to identify and get vital information on aircraft pairs in conflict. It gives you Time to Closest Point of Approach (TCPA), Quadrantal Direction (QDR), separation distance, Closest Point of Approach distance (DCPA), and Time of Loss of Separation (tLOS).\n",
    "    \n",
    "    Parameters:\n",
    "    - command: str (default 'SHOWTCPA')\n",
    "    \n",
    "    Example usage:\n",
    "    - GetConflictInfo('SHOWTCPA')\n",
    "    \n",
    "    Returns:\n",
    "    - str: conflict information between aircraft pairs\n",
    "    \"\"\"\n",
    "    client.send_event(b'STACK', 'SHOWTCPA')\n",
    "    time.sleep(1)\n",
    "    sim_output = update_until_complete(client)\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def ContinueMonitoring(time: str = '5'):\n",
    "    \"\"\"Monitor for conflicts between aircraft pairs for a specified time. \n",
    "    Parameters:\n",
    "    - time (str): The time in seconds to monitor for conflicts. Default is 5 seconds.\n",
    "    \n",
    "    Example usage:\n",
    "    - ContinueMonitoring('5')\n",
    "    \n",
    "    Returns:\n",
    "    - str: The conflict information between aircraft pairs throughout the monitoring period.\n",
    "    \"\"\"\n",
    "    for i in range(int(time)):\n",
    "        client.send_event(b'STACK', 'SHOWTCPA')\n",
    "        time.sleep(1)\n",
    "        sim_output += str(i) + ' sec: \\n' + update_until_complete(client) + '\\n'\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def SendCommand(command: str):\n",
    "    \"\"\"\n",
    "    Sends a command with optional arguments to the simulator and returns the output. \n",
    "    You can only send 1 command at a time.\n",
    "    \n",
    "    Parameters:\n",
    "    - command (str): The command to send to the simulator. Can only be a single command, with no AND or OR operators.\n",
    "    \n",
    "    Example usage:\n",
    "    - SendCommand('COMMAND_NAME ARG1 ARG2 ARG3 ... ARGn) # this command requires arguments\n",
    "    - SendCommand('COMMAND_NAME') # this command does not require arguments\n",
    "    \n",
    "    Returns:\n",
    "    str: The output from the simulator.\n",
    "    \"\"\"\n",
    "    # Convert the command and its arguments into a string to be sent\n",
    "    #command_with_args = ' '.join([command] + [str(arg) for arg in args])\n",
    "    # Send the command to the simulator\n",
    "    # client.update()  # Uncomment this if you need to update the client state before sending the command\n",
    "    print(command)\n",
    "    # replace \" or ' in the command string with nothing\n",
    "    command = command.replace('\"', '').replace(\"'\", \"\")\n",
    "    command = command.split('\\n')[0]\n",
    "    client.send_event(b'STACK', command)  \n",
    "    # wait 1 second\n",
    "    time.sleep(1)\n",
    "    # Wait for and retrieve the output from the simulator\n",
    "    sim_output = update_until_complete(client)\n",
    "    if sim_output == '':\n",
    "        return 'Command executed successfully.'\n",
    "    if 'Unknown command' in sim_output:\n",
    "        return sim_output + '\\n' + 'Please use a tool QueryDatabase to search for the correct command.'\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def QueryDatabase(input: str):\n",
    "    \"\"\"If you want to send command to a simulator please first search which command you should use. For example if you want to create an aircraft, search for 'how do I create an aircraft'.\n",
    "    Parameters:\n",
    "    - input: str (the query to search for)\n",
    "    Returns:\n",
    "    - list: the top 5 results from the database\n",
    "    \"\"\"\n",
    "\n",
    "    query_results = collection.query(\n",
    "        query_texts=[input],\n",
    "        n_results=5\n",
    "    )\n",
    "\n",
    "    return \"The HDG command sets the aircraft's heading, disengaging LNAV mode. Use it by specifying the aircraft ID and desired heading in degrees, like: HDG acid, hdg_degrees. The aircraft ID is the unique identifier of the aircraft you want to control, and the heading is the desired direction in degrees. For example, to set the heading of aircraft ABC to 90 degrees, you would use the command: HDG ABC, 90 \\n\\n\\n The ALT command adjusts an aircraft's altitude via autopilot, optionally setting vertical speed. Specify the aircraft ID, desired altitude in feet, and optionally, climb/descent speed in feet per minute, like ALT acid, alt, vspd. For example if you want to change aircraft KL123 height to 20000 ft the command is: ALT KL123 20000. or ALT KL123 FL200, or ALT KL123 FL200 10\"\n",
    "    #return query_results['documents'][0]\n",
    "\n",
    "\n",
    "tools = [GetAllAircraftInfo, GetConflictInfo,\n",
    "         SendCommand, QueryDatabase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"check if there are any conflicts between aircraft and if there are resolve them by changing altitude or heading. Your Goal is to have no conflicts between aircraft.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# Question: check if there are any conflicts between aircraft and if there are resolve them by changing altitude or heading. If there is no conflict then you don't need to do anything.\n",
    "# Thought: I should check if there are any conflicts between aircraft with the GetConflictInfo tool.\n",
    "# Action: GetConflictInfo\n",
    "# Action Input: SHOWTCPA\n",
    "# Observation: Conflict info ...\n",
    "# Thought: There is a conflict between these aircraft .... Let me check their information\n",
    "# Action: GetAllAircraftInfo\n",
    "# Action Input: GETACIDS\n",
    "# Observation: Aircraft info ...\n",
    "# Thought: Based on ... I should ... because ... to resolve the conflict  \n",
    "# Action: SendCommand\n",
    "# Action Input: COMMAND_NAME ARG1 ARG2 ARG3 ...\n",
    "# Observation: Command output ... I must confirm that the conflict is resolved\n",
    "# Action: GetConflictInfo\n",
    "# Action Input: SHOWTCPA\n",
    "# Observation: Conflict info ...\n",
    "# Thought: I now know the final answer\n",
    "# Final Answer: the final answer to the original input question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "from langchain import hub\n",
    "react_prompt = hub.pull(\"hwchase17/react\")\n",
    "react_prompt.template = \"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, specify exactly one of the following tools [{tool_names}]. No additional text or comments permitted.\n",
    "Action Input: the input argument to the action.\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\"\n",
    "react_prompt.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "openai_function_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "openai_function_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_json_chat_agent, create_xml_agent, create_openai_functions_agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_anthropic import AnthropicLLM, ChatAnthropic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # anthropic llm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llm = ChatAnthropic(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     model='claude-3-opus-20240229')  # Choose the LLM to use\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# llm = GoogleGenerativeAI(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     model=\"gemini-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "model_name = \"mixtral:8x22b-instruct\"\n",
    "llm = Ollama(base_url=\"https://ollama.junzis.com\", model=model_name)\n",
    "#llm = ChatOpenAI()\n",
    "def invoke_until_success(llm):\n",
    "    while True:\n",
    "        try:\n",
    "            # Attempt to invoke the method\n",
    "            llm.invoke('hi')\n",
    "            print(\"Invocation successful.\")\n",
    "            break  # Exit the loop if invocation was successful\n",
    "        except Exception as e:\n",
    "            print(\"Invocation failed, trying again...\")\n",
    "            time.sleep(1)  # Wait for a short period before retrying\n",
    "\n",
    "# Example usage\n",
    "# invoke_until_success(llm)\n",
    "invoke_until_success(llm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Construct the ReAct agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "react_agent = create_react_agent(llm, tools, react_prompt)\n",
    "\n",
    "\n",
    "openai_function_agent = create_openai_functions_agent(llm, tools, openai_function_prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "react_agent_executor = AgentExecutor(\n",
    "    agent=react_agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "\n",
    "openai_function_executor = AgentExecutor(\n",
    "    agent=openai_function_agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "#json_agent_executor = AgentExecutor(agent=json_agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.send_event(b'STACK', 'IC simple/conflicts/2ac/case1.scn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_until_complete(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "\n",
    "with tracing_v2_enabled(project_name=\"My Project\"):\n",
    "    out = react_agent_executor.invoke({\"input\": input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent, OpenAIFunctionsAgent, create_react_agent\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "# memory = ConversationBufferMemory(\n",
    "#     memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "message = SystemMessage(\n",
    "    content=(\n",
    "        \"You are very powerful air traffic controller, that can use commands to interact and control the airspace. \"\n",
    "        \"You can use tools to get information about aircrafts - get_all_aircraft_info, conflicts - get_conflict_info, send commands to the simulator to interact with aircraft - send_command_to_simulator and get information and search for commands in the database - query_database. If you are going to use send_command_to_simulator tool you must first search for the command you should use in the database. Search: how can I ...\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=\"chat_history\")],\n",
    ")\n",
    "\n",
    "\n",
    "# agent_kwargs = {\n",
    "#     \"extra_prompt_messages\": [MessagesPlaceholder(variable_name=\"chat_history\")],\n",
    "# }\n",
    "\n",
    "# # Create the memory\n",
    "# memory = ConversationBufferMemory(memory_key=\"memory\", return_messages=True)\n",
    "\n",
    "\n",
    "# Construct the OpenAI Tools agent\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"check if there are any conflicts between aircraft and if there are resolve them by changing altitude or heading\"\n",
    "while user_input != 'exit':\n",
    "    if user_input != 'exit':\n",
    "        # Invoke the agent and pass the current chat history\n",
    "        update_until_complete(client)\n",
    "        response = agent_executor.invoke(\n",
    "            {\"input\": user_input, \"chat_history\": chat_history})\n",
    "        human_message = HumanMessage(content=user_input)\n",
    "        ai_message = AIMessage(content=response['output'])\n",
    "        \n",
    "        chat_history.append(human_message)\n",
    "        chat_history.append(ai_message)\n",
    "        print(AIMessage(content=response['output']))\n",
    "        \n",
    "        time.sleep(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['intermediate_steps'][-1][0].log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({'input': \"what is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4-0125-preview\n",
    "# gpt-3.5-turbo-0125\n",
    "# gpt-3.5-turbo-instruct\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#output_parser = StrOutputParser()\n",
    "# Create a chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful air taffic control operator, that can also use tools to answer questions and perform tasks\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(conflict_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = prompt_template.format()\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = agent_executor.invoke({\"input\": input})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
