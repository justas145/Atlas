{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../bluesky')  # Adjust the path as necessary\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from prompts.prompts import conflict_prompt\n",
    "from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "from bluesky.network.client import Client\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from io import StringIO\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "\n",
    "    format_to_openai_tool_messages,\n",
    "\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "import bluesky\n",
    "import streamlit as st\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import tool, initialize_agent, AgentType, Tool\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now you can import bluesky modules\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-agent Collaboration\"\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-3-large\"\n",
    ")\n",
    "vectordb_path = 'C:/Users/justa/OneDrive/Desktop/Developer/LLM-Enhanced-ATM/llm/skills-library/vectordb'\n",
    "chroma_client = chromadb.PersistentClient(path=vectordb_path)\n",
    "# Get a collection object from an existing collection, by name. If it doesn't exist, create it.\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"test2\", embedding_function=openai_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "# capture output information from the bluesky client and return it as a string\n",
    "@contextmanager\n",
    "def capture_stdout():\n",
    "    new_stdout = StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = new_stdout\n",
    "    try:\n",
    "        yield new_stdout\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "def update_until_complete(client):\n",
    "    complete_output = \"\"\n",
    "    empty_output_count = 0  # Track consecutive empty outputs\n",
    "\n",
    "    while True:\n",
    "        with capture_stdout() as captured:\n",
    "            client.update()\n",
    "        new_output = captured.getvalue()\n",
    "\n",
    "        # Check if the current output is empty\n",
    "        if not new_output.strip():\n",
    "            empty_output_count += 1  # Increment counter for empty outputs\n",
    "        else:\n",
    "            empty_output_count = 0  # Reset counter if output is not empty\n",
    "            complete_output += new_output  # Add non-empty output to complete output\n",
    "\n",
    "        # If there are two consecutive empty outputs, break the loop\n",
    "        if empty_output_count >= 2:\n",
    "            break\n",
    "\n",
    "    # It's assumed you want to keep the last update outside the loop\n",
    "    client.update()\n",
    "\n",
    "    return complete_output\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "# connect to client\n",
    "client = Client()\n",
    "client.connect(\"127.0.0.1\", 11000, 11001)\n",
    "client.update()\n",
    "client.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Tuple, Union\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "\n",
    "# get all aircraft info\n",
    "# get conflict information\n",
    "\n",
    "@tool\n",
    "def GetAllAircraftInfo(command: str = 'GETACIDS'):\n",
    "    \"\"\"Get each aircraft information at current time: position, heading (deg), track (deg), altitude, V/S (vertical speed), calibrated, true and ground speed and mach number. Input is 'GETACIDS'.\n",
    "    \n",
    "    Parameters:\n",
    "    - command: str (default 'GETACIDS')\n",
    "    \n",
    "    Example usage:\n",
    "    - GetAllAircraftInfo('GETACIDS')\n",
    "    \n",
    "    Returns:\n",
    "    - str: all aircraft information\n",
    "    \"\"\"\n",
    "    command = command.replace('\"', '').replace(\"'\", \"\")\n",
    "    command = command.split('\\n')[0]\n",
    "    print(f'LLM input:{command}')\n",
    "\n",
    "    client.send_event(b'STACK', command)\n",
    "    time.sleep(1)\n",
    "    sim_output = update_until_complete(client)\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def GetConflictInfo(commad: str = 'SHOWTCPA'):\n",
    "    \"\"\"Use this tool to identify and get vital information on aircraft pairs in conflict. It gives you Time to Closest Point of Approach (TCPA), Quadrantal Direction (QDR), separation distance, Closest Point of Approach distance (DCPA), and Time of Loss of Separation (tLOS).\n",
    "    \n",
    "    Parameters:\n",
    "    - command: str (default 'SHOWTCPA')\n",
    "    \n",
    "    Example usage:\n",
    "    - GetConflictInfo('SHOWTCPA')\n",
    "    \n",
    "    Returns:\n",
    "    - str: conflict information between aircraft pairs\n",
    "    \"\"\"\n",
    "    client.send_event(b'STACK', 'SHOWTCPA')\n",
    "    time.sleep(1)\n",
    "    sim_output = update_until_complete(client)\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def ContinueMonitoring(duration: str = '5'):\n",
    "    \"\"\"Monitor for conflicts between aircraft pairs for a specified time. \n",
    "    Parameters:\n",
    "    - duration (str): The time in seconds to monitor for conflicts. Default is 5 seconds.\n",
    "    \n",
    "    Example usage:\n",
    "    - ContinueMonitoring('5')\n",
    "    \n",
    "    Returns:\n",
    "    - str: The conflict information between aircraft pairs throughout the monitoring period.\n",
    "    \"\"\"\n",
    "    for i in range(int(duration)):\n",
    "        client.send_event(b'STACK', 'SHOWTCPA')\n",
    "        time.sleep(1)\n",
    "        sim_output += str(i) + ' sec: \\n' + \\\n",
    "            update_until_complete(client) + '\\n'\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def SendCommand(command: str):\n",
    "    \"\"\"\n",
    "    Sends a command with optional arguments to the simulator and returns the output. \n",
    "    You can only send 1 command at a time.\n",
    "    \n",
    "    Parameters:\n",
    "    - command (str): The command to send to the simulator. Can only be a single command, with no AND or OR operators.\n",
    "    \n",
    "    Example usage:\n",
    "    - SendCommand('COMMAND_NAME ARG1 ARG2 ARG3 ...) # this command requires arguments\n",
    "    - SendCommand('COMMAND_NAME') # this command does not require arguments\n",
    "    \n",
    "    Returns:\n",
    "    str: The output from the simulator.\n",
    "    \"\"\"\n",
    "    # Convert the command and its arguments into a string to be sent\n",
    "    # command_with_args = ' '.join([command] + [str(arg) for arg in args])\n",
    "    # Send the command to the simulator\n",
    "    # client.update()  # Uncomment this if you need to update the client state before sending the command\n",
    "    print(command)\n",
    "    # replace \" or ' in the command string with nothing\n",
    "    command = command.replace('\"', '').replace(\"'\", \"\")\n",
    "    command = command.split('\\n')[0]\n",
    "    client.send_event(b'STACK', command)\n",
    "    # wait 1 second\n",
    "    time.sleep(1)\n",
    "    # Wait for and retrieve the output from the simulator\n",
    "    sim_output = update_until_complete(client)\n",
    "    if sim_output == '':\n",
    "        return 'Command executed successfully.'\n",
    "    if 'Unknown command' in sim_output:\n",
    "        return sim_output + '\\n' + 'Please use a tool QueryDatabase to search for the correct command.'\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def QueryDatabase(input: List[str]):\n",
    "    \"\"\"Use the tool to search for possible commands that can be sent to a simulator. The tool will return the top 5 commands that are similar to the input query. You can ask multiple questions at once.\n",
    "    Example usage:\n",
    "    - QueryDatabase(['Change heading of the aircraft', 'add waypoint to the aircraft'])\n",
    "    Output:\n",
    "    [[doc 1 for query 1, doc 2 for query 1, doc 3 for query 1], [doc 1 for query 2, doc 2 for query 2, ...], ...]\n",
    "    \"\"\"\n",
    "\n",
    "    query_results = collection.query(\n",
    "        query_texts=input,\n",
    "        n_results=5\n",
    "    )\n",
    "    \n",
    "    return query_results['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "with open('../prompts/basecmds.txt', 'r') as file:\n",
    "    base_cmds = file.read()\n",
    "\n",
    "openai_function_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "openai_function_prompt.messages[0].prompt.template = f\"\"\"\n",
    "\n",
    "These are the Base commands in simulation: {base_cmds}\n",
    "\n",
    "Your Task is:\n",
    "1.Analyze the Input Command: Understand and interpret the high-level command provided by the user.\n",
    "2. Identify Key Actions: Break down the command into its fundamental components or steps necessary to achieve the command.\n",
    "3. Associate Commands: For each identified step, determine the relevant commands from the base command set that will execute or facilitate the step.\n",
    "4. Formulate Step Descriptions: Write a concise description for each step, explaining its purpose and how it contributes to the overall task.\n",
    "5. Compile Command Syntax: List the exact commands for each step\n",
    "\n",
    "Input: Command\n",
    "Output format:\n",
    "Step-by-Step Plan for Command:\n",
    "- Step 1: Description of what needs to be accomplished in this step\n",
    "  Commands: [Command1, Command2, ...]\n",
    "- Step 2: Description of what needs to be accomplished in this step\n",
    "  Commands: [Command1, Command2, ...]\n",
    "- Step 3: [Description of what needs to be accomplished in this step]\n",
    "  Commands: [Command1, Command2, ...]\n",
    "- ...\n",
    "\n",
    "\n",
    "extra Commands: GetAllAircraftInfo - will output all aircraft information\n",
    "                GetConflictInfo - will output conflict information\n",
    "                ContinueMonitoring - will output conflict information for a specified time\n",
    "\n",
    "\"\"\"\n",
    "#openai_function_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent, create_json_chat_agent, create_xml_agent, create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", openai_function_prompt.messages[0].prompt.template),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        # Please see the how-to about improving performance with\n",
    "        # reference examples.\n",
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "# model='gpt-4-turbo-2024-04-09'\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Step by step plan and commands for each plan\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the plan.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Plan,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    step: Optional[list] = Field(\n",
    "        default=None, description=\"step number and description of the step\")\n",
    "    commands_lst: Optional[List[list]] = Field(\n",
    "        default=None, description=\"the names of the commands for the step\"\n",
    "    )\n",
    "\n",
    "\n",
    "chain1 = prompt1 | llm | StrOutputParser() | llm.with_structured_output(schema=Plan)\n",
    "\n",
    "# chain2 = {\"text\": chain1} | prompt2 |  llm.with_structured_output(schema=Plan)\n",
    "# chain2.invoke({'input': 'Resolve all conflicts in the airspace'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1.invoke({'input': 'Resolve all conflicts in the airspace'})\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent, OpenAIFunctionsAgent, create_react_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    # Each worker node will be given a name and some tools.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Verificator\", \"Monitor\", \"Controller\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH. Verificator - it verifies if there are still conflicts in the airspace after controller tried to resolve it. Monitor - it monitors the airspace and provides all aircraft detailed information. Controller - it resolves conflicts in the airspace. by sending commands to the simulator.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}. Verificator must always be before FINISH.\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "# gpt-3.5-turbo-0125\n",
    "# gpt-4-turbo-2024-04-09\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo-2024-04-09\")\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "# tools = [GetAllAircraftInfo, GetConflictInfo, ContinueMonitoring, SendCommand, QueryDatabase]\n",
    "\n",
    "monitor_prompt = \"You are part of the conflict detection and resolution team. You are responsible for monitoring the airspace for conflict and aircraft information.\"\n",
    "\n",
    "verificator_prompt = \"You are part of the conflict detection and resolution team. You are responsible for verifying that there are no conflicts in the airspace or that all conflicts have been resolved. You always must use Tool GetConflictInfo which will tell you exactly either conflict information if there are conflicts or 'No conflicts' if there are no conflicts.\"\n",
    "\n",
    "Controller_prompt = \"You are part of the conflict detection and resolution team. You are responsible for sending commands to the simulator and searching for possible commands, if you don't know what command to send. Tool SendCommand will send a command to the simulator and return the output. Tool QueryDatabase will search for possible commands that can be sent to the simulator.\"\n",
    "\n",
    "monitor_agent = create_agent(\n",
    "    llm, [GetAllAircraftInfo, GetConflictInfo, ContinueMonitoring], monitor_prompt)\n",
    "monitor_node = functools.partial(\n",
    "    agent_node, agent=monitor_agent, name=\"Monitor\")\n",
    "\n",
    "\n",
    "verification_agent = create_agent(\n",
    "    llm,\n",
    "    [GetAllAircraftInfo, GetConflictInfo, ContinueMonitoring],\n",
    "    \"You are responsable for verifying if the user request has been succesfully executed.\",\n",
    ")\n",
    "verification_node = functools.partial(\n",
    "    agent_node, agent=verification_agent, name=\"Verificator\")\n",
    "\n",
    "controller_agent = create_agent(\n",
    "    llm,\n",
    "    [SendCommand, QueryDatabase],\n",
    "    \"You are responsable for sending commands to the simulator and searching for possible commands.\",\n",
    ")\n",
    "controller_node = functools.partial(\n",
    "    agent_node, agent=controller_agent, name=\"Controller\")\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Monitor\", monitor_node)\n",
    "workflow.add_node(\"Verificator\", verification_node)\n",
    "workflow.add_node(\"Controller\", controller_node)\n",
    "workflow.add_node(\"supervisor\", supervisor_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for member in members:\n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.send_event(b'STACK', 'IC simple/conflicts/2ac/case3.scn')\n",
    "update_until_complete(client)\n",
    "update_until_complete(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_until_complete(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Please resolve any current conflict in the airspace\",)\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
