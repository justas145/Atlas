{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../bluesky')  # Adjust the path as neces\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from langchain.agents import tool, initialize_agent, AgentType, Tool\n",
    "from langchain.agents import tool\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from io import StringIO\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.agents import Tool\n",
    "from bluesky.network.client import Client\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_community.callbacks.streamlit import (\n",
    "    StreamlitCallbackHandler,\n",
    ")\n",
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_json_chat_agent, create_structured_chat_agent, create_openai_functions_agent\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(temperature=0.3, model_name=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.invoke('give me a plan to seperate two aircraft according to ICAO standards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialization\n",
    "vectordb_path = 'C:/Users/justa/OneDrive/Desktop/Developer/LLM-Enhanced-ATM/llm/skills-library/vectordb'\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-3-large\"\n",
    ")\n",
    "chroma_client = chromadb.PersistentClient(path=vectordb_path)\n",
    "\n",
    "# capture output information from the bluesky client and return it as a string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def capture_stdout():\n",
    "    new_stdout = StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = new_stdout\n",
    "    try:\n",
    "        yield new_stdout\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "def update_until_complete(client):\n",
    "    complete_output = \"\"\n",
    "    empty_output_count = 0  # Track consecutive empty outputs\n",
    "\n",
    "    while True:\n",
    "        with capture_stdout() as captured:\n",
    "            client.update()\n",
    "        new_output = captured.getvalue()\n",
    "\n",
    "        # Check if the current output is empty\n",
    "        if not new_output.strip():\n",
    "            empty_output_count += 1  # Increment counter for empty outputs\n",
    "        else:\n",
    "            empty_output_count = 0  # Reset counter if output is not empty\n",
    "            complete_output += new_output  # Add non-empty output to complete output\n",
    "\n",
    "        # If there are two consecutive empty outputs, break the loop\n",
    "        if empty_output_count >= 5:\n",
    "            break\n",
    "\n",
    "    # It's assumed you want to keep the last update outside the loop\n",
    "    client.update()\n",
    "\n",
    "    return complete_output\n",
    "\n",
    "\n",
    "# Streamlit UI\n",
    "\n",
    "# Select Vector DB\n",
    "collections = chroma_client.list_collections()\n",
    "collection_names = [collection.name for collection in collections]\n",
    "selected_collection = 'test2'\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=selected_collection, embedding_function=openai_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "\n",
    "# Connect to Simulator\n",
    "\n",
    "client = Client()\n",
    "client.connect(\"127.0.0.1\", 11000, 11001)\n",
    "client.update()\n",
    "client.update()\n",
    "\n",
    "### Tools ###\n",
    "\n",
    "\n",
    "# get all aircraft info\n",
    "# get conflict information\n",
    "\n",
    "@tool\n",
    "def GetAllAircraftInfo(command: str = 'GETACIDS'):\n",
    "    \"\"\"Get each aircraft information at current time: position, heading (deg), track (deg), altitude, V/S (vertical speed), calibrated, true and ground speed and mach number. Input is 'GETACIDS'.\n",
    "    \n",
    "    Parameters:\n",
    "    - command: str (default 'GETACIDS')\n",
    "    \n",
    "    Example usage:\n",
    "    - GetAllAircraftInfo('GETACIDS')\n",
    "    \n",
    "    Returns:\n",
    "    - str: all aircraft information\n",
    "    \"\"\"\n",
    "    command = command.replace('\"', '').replace(\"'\", \"\")\n",
    "    command = command.split('\\n')[0]\n",
    "    print(f'LLM input:{command}')\n",
    "\n",
    "    client.send_event(b'STACK', command)\n",
    "    time.sleep(1)\n",
    "    sim_output = update_until_complete(client)\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def GetConflictInfo(commad: str = 'SHOWTCPA'):\n",
    "    \"\"\"Use this tool to identify and get vital information on aircraft pairs in conflict. It gives you Time to Closest Point of Approach (TCPA), Quadrantal Direction (QDR), separation distance, Closest Point of Approach distance (DCPA), and Time of Loss of Separation (tLOS).\n",
    "    \n",
    "    Parameters:\n",
    "    - command: str (default 'SHOWTCPA')\n",
    "    \n",
    "    Example usage:\n",
    "    - GetConflictInfo('SHOWTCPA')\n",
    "    \n",
    "    Returns:\n",
    "    - str: conflict information between aircraft pairs\n",
    "    \"\"\"\n",
    "    client.send_event(b'STACK', 'SHOWTCPA')\n",
    "    client.send_event(b'STACK', 'GETACIDS')\n",
    "    time.sleep(1)\n",
    "    sim_output = update_until_complete(client)\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def ContinueMonitoring(duration: str = '5'):\n",
    "    \"\"\"Monitor for conflicts between aircraft pairs for a specified time. \n",
    "    Parameters:\n",
    "    - time (str): The time in seconds to monitor for conflicts. Default is 5 seconds.\n",
    "    \n",
    "    Example usage:\n",
    "    - ContinueMonitoring('5')\n",
    "    \n",
    "    Returns:\n",
    "    - str: The conflict information between aircraft pairs throughout the monitoring period.\n",
    "    \"\"\"\n",
    "    sim_output = ''\n",
    "    for i in range(int(duration)):\n",
    "        client.send_event(b'STACK', 'SHOWTCPA')\n",
    "        time.sleep(1)\n",
    "        sim_output += str(i) + ' sec: \\n' + \\\n",
    "            update_until_complete(client) + '\\n'\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def SendCommand(command: str):\n",
    "    \"\"\"\n",
    "    Sends a command with optional arguments to the simulator and returns the output. \n",
    "    You can only send 1 command at a time.\n",
    "    \n",
    "    Parameters:\n",
    "    - command (str): The command to send to the simulator. Can only be a single command, with no AND or OR operators.\n",
    "    \n",
    "    Example usage:\n",
    "    - SendCommand('COMMAND_NAME ARG1 ARG2 ARG3 ...) # this command requires arguments\n",
    "    - SendCommand('COMMAND_NAME') # this command does not require arguments\n",
    "    \n",
    "    Returns:\n",
    "    str: The output from the simulator.\n",
    "    \"\"\"\n",
    "    # Convert the command and its arguments into a string to be sent\n",
    "    # command_with_args = ' '.join([command] + [str(arg) for arg in args])\n",
    "    # Send the command to the simulator\n",
    "    # client.update()  # Uncomment this if you need to update the client state before sending the command\n",
    "    print(command)\n",
    "    # replace \" or ' in the command string with nothing\n",
    "    command = command.replace('\"', '').replace(\"'\", \"\")\n",
    "    command = command.split('\\n')[0]\n",
    "    client.send_event(b'STACK', command)\n",
    "    # wait 1 second\n",
    "    time.sleep(1)\n",
    "    # Wait for and retrieve the output from the simulator\n",
    "    sim_output = update_until_complete(client)\n",
    "    if sim_output == '':\n",
    "        return 'Command executed successfully.'\n",
    "    if 'Unknown command' in sim_output:\n",
    "        return sim_output + '\\n' + 'Please use a tool QueryDatabase to search for the correct command.'\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def QueryDatabase(input: str):\n",
    "    \"\"\"If you want to send command to a simulator please first search which command you should use. For example if you want to create an aircraft, search for 'how do I create an aircraft'.\n",
    "    Parameters:\n",
    "    - input: str (the query to search for)\n",
    "    Returns:\n",
    "    - list: the top 5 results from the database\n",
    "    \"\"\"\n",
    "\n",
    "    query_results = collection.query(\n",
    "        query_texts=[input],\n",
    "        n_results=5\n",
    "    )\n",
    "    return query_results\n",
    "    return \"The HDG command sets the aircraft's heading, disengaging LNAV mode. Use it by specifying the aircraft ID and desired heading in degrees, like: HDG acid, hdg_degrees. The aircraft ID is the unique identifier of the aircraft you want to control, and the heading is the desired direction in degrees. For example, to set the heading of aircraft ABC to 90 degrees, you would use the command: HDG ABC, 90 \\n\\n\\n The ALT command adjusts an aircraft's altitude via autopilot, optionally setting vertical speed. Specify the aircraft ID, desired altitude in feet, and optionally, climb/descent speed in feet per minute, like ALT acid, alt, vspd. For example if you want to change aircraft KL123 height to 20000 ft the command is: ALT KL123 20000. or ALT KL123 FL200, or ALT KL123 FL200 10\"\n",
    "    # return query_results['documents'][0]\n",
    "\n",
    "\n",
    "tools = [GetAllAircraftInfo, GetConflictInfo,\n",
    "         SendCommand, QueryDatabase, ContinueMonitoring]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "chat = ChatGroq(temperature=0.2, model_name=\"llama3-70b-8192\")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent = create_openai_tools_agent(chat, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = agent_executor.invoke({'input': 'create 2 aircrafts', 'chat_history': [HumanMessage(content='hi'), AIMessage(content='hi')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['intermediate_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intermediate_steps(data):\n",
    "    steps_summary = []\n",
    "    if 'intermediate_steps' in data:\n",
    "        for step in data['intermediate_steps']:\n",
    "            action = step[0].tool\n",
    "            command = json.dumps(step[0].tool_input)\n",
    "            # Extracting the response message from log\n",
    "            invoke = step[0].log\n",
    "            response = step[1]\n",
    "\n",
    "            summary = f\"Action: {action}, Command: {command}, Invoke: {invoke}, Response: {response}\"\n",
    "            steps_summary.append(summary)\n",
    "\n",
    "    return '\\n\\n'.join(steps_summary)\n",
    "\n",
    "import json\n",
    "\n",
    "intermediate_steps_str = get_intermediate_steps(out)\n",
    "\n",
    "# for step in out['intermediate_steps']:\n",
    "#     print(step[0].tool)\n",
    "#     print(json.dumps(step[0].tool_input))\n",
    "#     print(step[0].log)\n",
    "#     print(step[1])\n",
    "#     print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_steps_str"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
