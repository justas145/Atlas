{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import tool, initialize_agent, AgentType, Tool\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append('../../bluesky')  # Adjust the path as necessary\n",
    "\n",
    "\n",
    "# Now you can import bluesky modules\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "import bluesky\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, SystemMessage\n",
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "\n",
    "    format_to_openai_tool_messages,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from bluesky.network.client import Client\n",
    "\n",
    "from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "\n",
    "from prompts.prompts import conflict_prompt\n",
    "\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from langchain.agents import Tool\n",
    "\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Communication-Errors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model_name=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "vectordb_path = os.getcwd() + '/../skills-library/vectordb'\n",
    "#vectordb_path = 'C:/Users/justa/OneDrive/Desktop/Developer/LLM-Enhanced-ATM/llm/skills-library/vectordb'\n",
    "chroma_client = chromadb.PersistentClient(path=vectordb_path)\n",
    "# Get a collection object from an existing collection, by name. If it doesn't exist, create it.\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"test1\", embedding_function=openai_ef, metadata={\"hnsw:space\": \"cosine\"})\n",
    "collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture output information from the bluesky client and return it as a string\n",
    "@contextmanager\n",
    "def capture_stdout():\n",
    "    new_stdout = StringIO()\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = new_stdout\n",
    "    try:\n",
    "        yield new_stdout\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "def update_until_complete(client):\n",
    "    complete_output = \"\"\n",
    "    empty_output_count = 0  # Track consecutive empty outputs\n",
    "\n",
    "    while True:\n",
    "        with capture_stdout() as captured:\n",
    "            client.update()\n",
    "        new_output = captured.getvalue()\n",
    "\n",
    "        # Check if the current output is empty\n",
    "        if not new_output.strip():\n",
    "            empty_output_count += 1  # Increment counter for empty outputs\n",
    "        else:\n",
    "            empty_output_count = 0  # Reset counter if output is not empty\n",
    "            complete_output += new_output  # Add non-empty output to complete output\n",
    "\n",
    "        # If there are two consecutive empty outputs, break the loop\n",
    "        if empty_output_count >= 3:\n",
    "            break\n",
    "\n",
    "    # It's assumed you want to keep the last update outside the loop\n",
    "    client.update()\n",
    "\n",
    "    return complete_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to client\n",
    "client = Client()\n",
    "client.connect(\"127.0.0.1\", 11000, 11001)\n",
    "client.update()\n",
    "client.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../prompts/basecmds.txt', 'r') as file:\n",
    "    base_cmds = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tools for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get all aircraft info\n",
    "# get conflict information\n",
    "\n",
    "@tool\n",
    "def GetAllAircraftInfo(command: str = 'GETACIDS'):\n",
    "    \"\"\"Get each aircraft information at current time: position, heading (deg), track (deg), altitude, V/S (vertical speed), calibrated, true and ground speed and mach number. Input is 'GETACIDS'.\n",
    "    \n",
    "    Parameters:\n",
    "    - command: str (default 'GETACIDS')\n",
    "    \n",
    "    Example usage:\n",
    "    - GetAllAircraftInfo('GETACIDS')\n",
    "    \n",
    "    Returns:\n",
    "    - str: all aircraft information\n",
    "    \"\"\"\n",
    "    command = command.replace('\"', '').replace(\"'\", \"\")\n",
    "    command = command.split('\\n')[0]\n",
    "    print(f'LLM input:{command}')\n",
    "    \n",
    "    client.send_event(b'STACK', command)\n",
    "    time.sleep(1)\n",
    "    sim_output = update_until_complete(client)\n",
    "    return sim_output\n",
    "\n",
    "@tool\n",
    "def GetConflictInfo(commad: str = 'SHOWTCPA'):\n",
    "    \"\"\"Use this tool to identify and get vital information on aircraft pairs in conflict. It gives you Time to Closest Point of Approach (TCPA), Quadrantal Direction (QDR), separation distance, Closest Point of Approach distance (DCPA), and Time of Loss of Separation (tLOS).\n",
    "    \n",
    "    Parameters:\n",
    "    - command: str (default 'SHOWTCPA')\n",
    "    \n",
    "    Example usage:\n",
    "    - GetConflictInfo('SHOWTCPA')\n",
    "    \n",
    "    Returns:\n",
    "    - str: conflict information between aircraft pairs\n",
    "    \"\"\"\n",
    "    client.send_event(b'STACK', 'SHOWTCPA')\n",
    "    time.sleep(1)\n",
    "    sim_output = update_until_complete(client)\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def ContinueMonitoring(time: str = '5'):\n",
    "    \"\"\"Monitor for conflicts between aircraft pairs for a specified time. \n",
    "    Parameters:\n",
    "    - time (str): The time in seconds to monitor for conflicts. Default is 5 seconds.\n",
    "    \n",
    "    Example usage:\n",
    "    - ContinueMonitoring('5')\n",
    "    \n",
    "    Returns:\n",
    "    - str: The conflict information between aircraft pairs throughout the monitoring period.\n",
    "    \"\"\"\n",
    "    for i in range(int(time)):\n",
    "        client.send_event(b'STACK', 'SHOWTCPA')\n",
    "        time.sleep(1)\n",
    "        sim_output += str(i) + ' sec: \\n' + update_until_complete(client) + '\\n'\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "@tool\n",
    "def SendCommand(command: str):\n",
    "    \"\"\"\n",
    "    Sends a command with optional arguments to the simulator and returns the output. \n",
    "    You can only send 1 command at a time.\n",
    "    \n",
    "    Parameters:\n",
    "    - command (str): The command to send to the simulator. Can only be a single command, with no AND or OR operators.\n",
    "    \n",
    "    Example usage:\n",
    "    - SendCommand('COMMAND_NAME ARG1 ARG2 ARG3 ...) # this command requires arguments\n",
    "    - SendCommand('COMMAND_NAME') # this command does not require arguments\n",
    "    \n",
    "    Returns:\n",
    "    str: The output from the simulator.\n",
    "    \"\"\"\n",
    "    # Convert the command and its arguments into a string to be sent\n",
    "    #command_with_args = ' '.join([command] + [str(arg) for arg in args])\n",
    "    # Send the command to the simulator\n",
    "    # client.update()  # Uncomment this if you need to update the client state before sending the command\n",
    "    print(command)\n",
    "    # replace \" or ' in the command string with nothing\n",
    "    command = command.replace('\"', '').replace(\"'\", \"\")\n",
    "    command = command.split('\\n')[0]\n",
    "    client.send_event(b'STACK', command)  \n",
    "    # wait 1 second\n",
    "    time.sleep(1)\n",
    "    # Wait for and retrieve the output from the simulator\n",
    "    sim_output = update_until_complete(client)\n",
    "    if sim_output == '':\n",
    "        return 'Command executed successfully.'\n",
    "    if 'Unknown command' in sim_output:\n",
    "        return sim_output + '\\n' + 'Please use a tool QueryDatabase to search for the correct command.'\n",
    "    return sim_output\n",
    "\n",
    "\n",
    "\n",
    "@tool\n",
    "def QueryDatabase(input: str):\n",
    "    \"\"\"If you want to send command to a simulator please first search which command you should use. For example if you want to create an aircraft, search for 'how do I create an aircraft'.\n",
    "    Parameters:\n",
    "    - input: str (the query to search for)\n",
    "    Returns:\n",
    "    - list: the top 10 results from the database\n",
    "    \"\"\"\n",
    "\n",
    "    query_results = collection.query(\n",
    "        query_texts=[input],\n",
    "        n_results=5\n",
    "    )\n",
    "\n",
    "    \n",
    "    return query_results['documents'][0]\n",
    "\n",
    "\n",
    "tools = [GetAllAircraftInfo, GetConflictInfo,\n",
    "         SendCommand, QueryDatabase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"check if there are any conflicts between aircraft and if there are resolve them by changing altitude or heading. Your Goal is to have no conflicts between aircraft.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "from langchain import hub\n",
    "react_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "react_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# react_prompt.template = \"\"\"\n",
    "# Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "# {tools}\n",
    "\n",
    "# Use the following format:\n",
    "\n",
    "# Question: the input question you must answer\n",
    "# Thought: you should always think about what to do\n",
    "# Action: the action to take, specify exactly one of the following tools [{tool_names}]. No additional text or comments permitted.\n",
    "# Action Input: the input argument to the action.\n",
    "# Observation: the result of the action\n",
    "# ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "# Thought: I now know the final answer\n",
    "# Final Answer: the final answer to the original input question\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Begin!\n",
    "\n",
    "# Question: {input}\n",
    "# Thought:{agent_scratchpad}\n",
    "# \"\"\"\n",
    "# react_prompt.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_prompt = hub.pull(\"hwchase17/xml-agent-convo\")\n",
    "xml_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "openai_function_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "openai_function_prompt.messages[0].prompt.template = \"\"\"\n",
    "You are an elite air traffic assistant that is monitoring the communication between the pilots and the air traffic controller. Your task is to detect call sign errors, ambiguities, misinterpretation and if you detect any, provide a clarification to the pilots or the air traffic controller. Provide clarification message in this format: Sender: LLM, Receiver: <Here you decide who should receive the message, can be multiple receivers>, message: <Your Message>. After providing the message try to fix the situation by using tools. Use these tools:\n",
    "GetAllAircraftInfo - This will output all the information about current aircraft in the airspace\n",
    "GetConflictInfo - This will output the information about any conflicts between aircraft\n",
    "SendCommand - This will send a command to the simulator. You can use this to control the aircraft\n",
    "QueryDatabase - This will search the database for the correct command to send to the simulator. Use this if you are unsure of the command to send to the simulator.\n",
    "\n",
    "Be a proactive air traffic assistant, use tools as much as possible to get the information you need to provide the correct clarification.\n",
    "\n",
    "Examples of call sign errors:\n",
    "- similar call signs: \"DAL123\" and \"DAL132\". Command is for \"DAL123\" but \"DAL132\" responded or both responded.\n",
    "- Incomplete call signs: command sent to \"Delta123\" instead of \"DAL123 Heavy\"\n",
    "- Readback call sign error: command sent to \"XYZ123\" but \"XYZ132\" readback the command as \"XYZ123\" or \"XYZ123\" readback the command as \"XYZ132\"\n",
    "- Misheard call sign: command sent to \"ABC123\" but \"ABC123\" heard it as \"ABC132\" so \"ABC123\" never responded.\n",
    "- Wrong prefix: command sent to \"American123\" but was meant for \"Delta123\"\n",
    "- wrong suffix: command sent to \"KLM123\" but was meant for \"KLM123Heavy\"\n",
    "- Misheard Call Sign: command sent to ABC123 but ABC123 misheard as another call sign and it didn't respond.\n",
    "\n",
    "\n",
    "Notes:\n",
    "there could be commands to aircraft that don't even exits and was meant to be for another aircraft creating a call sign error. for example \"AAL123\" instead of \"DAL123\", so you always need to check which aircraft currently exist in airspace with GetAllAircraftInfo tool.\n",
    "\n",
    "If the command is clear, just say \"Command is clear\".\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "openai_function_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, create_json_chat_agent, create_xml_agent, create_openai_functions_agent\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_anthropic import AnthropicLLM, ChatAnthropic\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "#     model=\"gemini-pro\", google_api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=OpenAI(), max_token_limit=1000, memory_key=\"chat_history\", return_messages=True)\n",
    "# model_name = \"llama2:70b\"\n",
    "# llm = Ollama(base_url=\"https://ollama.junzis.com\", model=model_name)\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-2024-04-09')\n",
    "def invoke_until_success(llm):\n",
    "    while True:\n",
    "        try:\n",
    "            # Attempt to invoke the method\n",
    "            llm.invoke('hi')\n",
    "            print(\"Invocation successful.\")\n",
    "            break  # Exit the loop if invocation was successful\n",
    "        except Exception as e:\n",
    "            print(\"Invocation failed, trying again...\")\n",
    "            time.sleep(1)  # Wait for a short period before retrying\n",
    "\n",
    "# Example usage\n",
    "# invoke_until_success(llm)\n",
    "#invoke_until_success(llm)\n",
    "\n",
    "react_agent = create_react_agent(llm, tools, react_prompt)\n",
    "\n",
    "xml_agent = create_xml_agent(llm, tools, xml_prompt)\n",
    "\n",
    "openai_function_agent = create_openai_functions_agent(llm, tools, openai_function_prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "react_agent_executor = AgentExecutor(\n",
    "    agent=react_agent, tools=tools, verbose=True, handle_parsing_errors=True, memory=memory)\n",
    "\n",
    "xml_agent_executor = AgentExecutor(\n",
    "    agent=xml_agent, tools=tools, verbose=True, handle_parsing_errors=True, memory=memory)\n",
    "\n",
    "openai_function_executor = AgentExecutor(\n",
    "    agent=openai_function_agent, tools=tools, verbose=True, handle_parsing_errors=True, memory=memory)\n",
    "#json_agent_executor = AgentExecutor(agent=json_agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.send_event(b'STACK', 'IC simple/communication/call_sign_errors/readback.scn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_until_complete(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# # get time in hh:mm:sec\n",
    "# def get_time():\n",
    "#     return time.strftime(\"%H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_time = get_time()\n",
    "# input = f\"TIME: {current_time} SOURCE: BA023 MESSAGE: VR342, roger, continuing taxi, holding short of Runway 27.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "\n",
    "# def read_csv_to_list(filename):\n",
    "#     # List to store the formatted data\n",
    "#     formatted_data = []\n",
    "\n",
    "#     # Open the CSV file\n",
    "#     with open(filename, newline='', encoding='utf-8') as csvfile:\n",
    "#         # Create a CSV reader specifying the delimiter as '|'\n",
    "#         reader = csv.DictReader(csvfile, delimiter='|')\n",
    "\n",
    "#         # Iterate over each row in the CSV file\n",
    "#         for row in reader:\n",
    "#             # Format each row as specified and add it to the list\n",
    "#             formatted_row = f\"time: {row['time']}, source: {row['source']}, message: {row['message']}\"\n",
    "#             formatted_data.append(formatted_row)\n",
    "\n",
    "#     return formatted_data\n",
    "\n",
    "\n",
    "# formatted_data = read_csv_to_list('../data/conversations/tenerife.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_list(file_path):\n",
    "    \"\"\"\n",
    "    Reads a text file and returns a list of lines.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The path to the text file to be read.\n",
    "\n",
    "    Returns:\n",
    "    list: A list where each element is a line from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()  # Read all lines in the file\n",
    "        # Strip whitespace from each line\n",
    "        lines = [line.strip() for line in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "lines = read_file_to_list(\n",
    "    \"../data/conversations/call_sign_errors/readback.txt\")\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lines)):\n",
    "    print(lines[i])\n",
    "    out = openai_function_executor.invoke({\"input\": lines[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# langsmith knows which client by api from .env file\n",
    "langsmith_client = langsmith.Client()\n",
    "\n",
    "# Load the chat history from Langsmith\n",
    "chat_history_df = langsmith_client.get_test_results(\n",
    "    project_name=\"Communication-Errors\")\n",
    "# Filter chat history to only include questions where the model did not know the answer by searching 'sorry' in the answer\n",
    "chat_history_df = chat_history_df[['input.input', 'outputs.output']]\n",
    "# remove rows with nan\n",
    "chat_history_df = chat_history_df.dropna()\n",
    "# reset index\n",
    "chat_history_df = chat_history_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# only keep rows 1 to 169\n",
    "chat_history_df = chat_history_df.iloc[0:96]\n",
    "\n",
    "# reorder from bottom to top\n",
    "chat_history_df = chat_history_df.iloc[::-1]\n",
    "\n",
    "# save to csv, seperator is |\n",
    "\n",
    "\n",
    "\n",
    "chat_history_df.to_csv('../data/conversation_with_llm/tenerife.csv', index=False, sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
